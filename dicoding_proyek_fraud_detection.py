# -*- coding: utf-8 -*-
"""Dicoding_Proyek Fraud Detection_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yxXm_IjUL15ipSNADNSZoLqq3NMkLQs9

# **Proyek: Fraud Detection**

# **Latar Belakang**

Dalam era digital, transaksi kartu kredit telah menjadi salah satu metode pembayaran yang paling umum digunakan. Kemudahan, kecepatan, dan fleksibilitasnya menjadikan kartu kredit alat transaksi utama bagi banyak individu dan bisnis. Namun, peningkatan penggunaan kartu kredit juga membawa tantangan serius, yaitu risiko terjadinya fraud (penipuan). Fraud pada transaksi kartu kredit dapat berupa pencurian data kartu, penggunaan kartu tanpa izin, hingga penyalahgunaan oleh pihak yang tidak bertanggung jawab. Kerugian yang diakibatkan oleh tindakan ini sangat signifikan, baik bagi pengguna kartu kredit maupun lembaga keuangan.

Menurut laporan industri, kerugian global akibat fraud kartu kredit mencapai miliaran dolar setiap tahunnya. Selain kerugian finansial, kepercayaan pelanggan terhadap penyedia layanan keuangan juga dapat terpengaruh. Oleh karena itu, mendeteksi dan mencegah fraud dengan cepat dan akurat menjadi prioritas utama bagi penyedia layanan kartu kredit.

Tradisionalnya, deteksi fraud dilakukan menggunakan aturan berbasis logika (rule-based systems). Namun, pendekatan ini memiliki keterbatasan, seperti kesulitan menangani volume data yang besar, pola fraud yang dinamis, serta potensi tingginya false positive (transaksi yang sah tetapi terdeteksi sebagai fraud) dan false negative (transaksi fraud tetapi terdeteksi sebagai sah). Dalam konteks ini, machine learning muncul sebagai solusi yang lebih adaptif dan efektif.

Machine learning memungkinkan model untuk belajar dari data historis transaksi, baik yang valid maupun fraud. Dengan memanfaatkan algoritma seperti random forest, sistem deteksi fraud dapat mengenali pola-pola kompleks yang sulit dideteksi oleh metode konvensional. Model machine learning juga mampu terus diperbarui seiring bertambahnya data, sehingga lebih adaptif terhadap metode fraud yang baru.

Penerapan machine learning untuk deteksi fraud pada transaksi kartu kredit tidak hanya meningkatkan akurasi pendeteksian, tetapi juga membantu mengurangi waktu respons dalam mencegah kerugian yang lebih besar. Oleh karena itu, penelitian dan pengembangan di bidang ini menjadi sangat penting, baik untuk meningkatkan efisiensi sistem keuangan maupun melindungi konsumen dari risiko kejahatan siber.

Sebuah penelitian menggunakan Random Forest yang digabungkan dengan Synthetic Minority Oversampling Technique (SMOTE) berhasil mencapai skor F1 sekitar 98%. Metode ini dirancang untuk mengatasi tantangan data tidak seimbang dalam mendeteksi transaksi penipuan. Penelitian ini juga menunjukkan bahwa penyetelan hyperparameter dapat meningkatkan kinerja model secara signifikan [1].

Dalam penelitian yang membandingkan Random Forest dengan model neural networks untuk deteksi penipuan kartu kredit, Random Forest mencatat skor F1 makro sebesar 96%. Penelitian ini juga menyoroti keunggulan RF dalam hal interpretabilitas dan efisiensi komputasi dibandingkan model yang lebih kompleks [2].

Referensi :\
[1] [Credit Card Fraud Detection Using Enhanced Random Forest Classifier for Imbalanced Data](https://arxiv.org/abs/2303.06514)\
[2] [Analysis of Credit Card Fraud Detection Performance Using Random Forest Classifier & Neural Networks Model](https://everant.org/index.php/etj/article/download/1196/848/3334)

# **Business Understanding**

## **Problem Statements**

- **Keamanan Sistem Pembayaran :** Penipuan kartu kredit terus meningkat, merugikan institusi keuangan secara signifikan dan merusak integritas sistem pembayaran.
- **Keterbatasan Rule-Based System :** Metode deteksi penipuan berbasis aturan (rule-based system) memiliki keterbatasan dalam mengenali pola penipuan yang kompleks dan dinamis. Sistem ini sulit beradaptasi terhadap ancaman baru, membutuhkan pembaruan manual yang memakan waktu, dan sering kali menghasilkan banyak false positives maupun false negatives.
- **False Positives :** Tingginya transaksi valid yang salah ditandai sebagai penipuan dapat mengganggu pengalaman pelanggan dan menurunkan loyalitas mereka.
- **False Negatives :** Transaksi penipuan yang tidak terdeteksi (false negatives) dapat menyebabkan kerugian finansial yang besar dan mencederai reputasi bisnis karena ketidakmampuan sistem mendeteksi ancaman yang nyata.

## **Goals**

- **Keamanan Sistem Pembayaran :** Meminimalkan kerugian finansial akibat penipuan dengan deteksi yang lebih akurat dan cepat.
- **Mengatasi Keterbatasan Rule-Based System :** Menggunakan machine learning untuk mendeteksi pola penipuan yang lebih kompleks dan dinamis, serta meningkatkan efisiensi dan akurasi deteksi penipuan tanpa memerlukan pembaruan aturan secara manual.
- **Mengurangi False Positives :** Menurunkan jumlah transaksi valid yang salah ditandai sebagai penipuan untuk menjaga pengalaman pelanggan yang baik.
- **Mengurangi False Negatives :** Memastikan deteksi penipuan yang lebih akurat dengan meminimalkan jumlah transaksi penipuan yang lolos dari pengawasan, sehingga mengurangi kerugian dan melindungi reputasi bisnis.

## **Solutions**

- Menggunakan random forest sebagai algoritma machine learning yang memiliki performa tinggi dan lebih robust dalam menangani data imbalance.
- Melakukan hyperparameter tuning pada model random forest untuk mendapatkan model yang lebih sesuai dengan data sehingga performa meningkat.
- Menggunakan SMOTE untuk oversampling data sehingga mengurangi data imbalance.
- Menggunakan ensemble learning dengan cara menggabungkan dua model random forest secara cascade untuk meningkatkan performa klasifikasi.

# **Datasets**

Dataset credit card transaction menyediakan catatan terperinci tentang transaksi kartu kredit, termasuk informasi tentang waktu transaksi, jumlah, dan detail pribadi serta pedagang terkait. Kumpulan data ini memiliki sekitar 1,3 juta baris.
[Kaggle](https://www.kaggle.com/datasets/priyamchoksi/credit-card-transactions-dataset)

**Atribut Dataset**
*   **Unnamed: 0 :** indeks transaksi
*   **trans_date_trans_time	:** waktu dari transaction.
*   **cc_num :** nomor kartu kredit
*   **merchant :** toko tempat transaksi terjadi
*   **category :** tipe transaksi
*   **amt :** jumlah transaksi
*   **first :** nama depan dari pemegang kartu kredit
*   **last :** nama belakang dari pemegang kartu kredit
*   **gender :** gender dari pemegang kartu kredit
*   **street :** nama jalan dari pemegang kartu kredit
*   **city :** nama kota dari pemegang kartu kredit
*   **state :** nama negara bagian dari pemegang kartu kredit
*   **zip :** kode pos dari pemegang kartu kredit
*   **lat :** posisi garis lintang ketika transaksi
*   **long :** posisi garis bujur ketika transaksi
*   **city_pop :** populasi kota ketika transaksi terjadi
*   **job :** pekerjaan dari pemegang kartu kredit
*   **dob :** Tanggal lahir dari pemegang kartu kredit
*   **trans_num :** nomor unik transaksi
*   **unix_time :** waktu dalam format unix
*   **merch_lat :** posisi garis lintang dari toko
*   **merch_long :** posisi garis bujur dari toko
*   **is_fraud :** indikator apakah transaksi fraud (1) atau non-fraud (0)
*   **merch_zipcode :** kode pos dari toko

# **Import Library**
"""

# Install library
import kagglehub
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from imblearn.over_sampling import SMOTE
import copy
from itertools import product
import joblib

from imblearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay

pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', 300)

from google.colab import drive
drive.mount('/content/drive')

"""# **Load Data**"""

# Download latest version
path = kagglehub.dataset_download("priyamchoksi/credit-card-transactions-dataset")

print("Path to dataset files:", path)

df_dataset = pd.read_csv(path + '/credit_card_transactions.csv')
df_X = df_dataset.drop(columns=['is_fraud'])
df_y = df_dataset['is_fraud']

df_dataset.head()

"""# **Exploratory Data Analysis (EDA)**

## **Info Data**
"""

# Cek jumlah missing value dan jenis data setiap atribut
df_dataset.info()

"""* Dataset memiliki 1296675 data dengan 24 atribut
* Dataset terdiri dari atribut numerik dan kategorikal
* Dataset memiliki missing value pada atribut **merch_zipcode**
"""

# Rangkuman statistik dari atribut numerik
df_X.describe()

# Cek apakah dataset memiliki record yang duplikat
df_X.duplicated().sum()

"""Dataset tidak memiliki record yang duplikat"""

# Mencari atribut numerik dan kategorical
num_cols = df_X.select_dtypes(include=['int64', 'float64']).columns
cat_cols = df_X.select_dtypes(include=['object']).columns

"""## **Checking Missing Value**"""

def check_missing(df_dataset):
  # Jumlah missing values
  missing_values = df_dataset.isnull().sum()
  # Persentase missing values
  missing_percentage = (missing_values / len(df_dataset)) * 100
  # Concatenate the count and percentage of missing values
  missing_data = pd.concat([missing_values, missing_percentage], axis=1, keys=['Missing Values', 'Percentage'])
  # Sort berdasarkan jumlah missing value
  missing_data = missing_data[missing_data['Missing Values'] > 0].sort_values(by='Missing Values', ascending=False)
  # Ambil index
  missing_data['Atributes'] = missing_data.index
  # Jenis column
  missing_data['Data Type'] = df_dataset[missing_data['Atributes']].dtypes
  # Urutkan column
  missing_data = missing_data[['Atributes', 'Data Type', 'Missing Values', 'Percentage']]
  missing_data = missing_data.reset_index(drop=True)
  return missing_data

check_missing(df_X)

"""Dataset memiliki missing value pada atribut **merch_zipcode** sebanyak sekitar 15%"""

(df_X == 0).sum()

"""* Tidak ada data nol untuk seluruh atribut yang dapat berarti missing value
* Data nol hanya pada kolom **unnamed (id)**

## **Checking Outliers**
"""

# Buat boxplot untuk seluruh atribut numerik
n_cols = 6
n_rows = (len(num_cols) // n_cols) + 1

plt.figure(figsize = (12, 6))
for i, num in enumerate(num_cols):
  plt.subplot(n_rows, n_cols, i+1)
  sns.boxplot(df_X[num])
plt.tight_layout()
plt.show()

"""Atribut numerik terlihat seperti memiliki outlier secara statistik. Namun, outlier tersebut terdapat pada data posisi (**zip, lat, long, merch_zip, merch_lat, merch_long**), populasi (**city_pop**), nomor kartu kredit (**cc_num**), dan jumlah transaksi (**amt**) di mana data tersebut memiliki secara natural memiliki nilai yang sangat bervariasi sehingga outlier tersebut dapat diabaikan.

## **Univariate Analysis**

### **Numerik**

Pengecekan histogram hanya dilakukan pada fitur **amt** dan **city_pop** karena fitur numerik lain seperti **zip, lat, long, merch_zip, merch_lat, merch_long** menandakan posisi sehingga tidak relevan apabila nilainya dicari distribusinya dalam bentuk histogram
"""

# Histogram untuk kolom numerik
df_X[['amt', 'city_pop']].hist(bins=50, figsize=(10, 3), layout=(1, 2))
plt.show()

"""Berdasarkan histogram diatas tampak bahwa sebagian besar transaksi terjadi pada jumlah transaksi (**amt**) yang cenderung rendah dan pada kota dengan populasi (**city_pop**) rendah

### **Kategorikal**
"""

# Jumlah kategori unik dalam setiap kelas kategori
for col in cat_cols:
  print(col, df_dataset[col].nunique())

"""Pengecekan barplot hanya dilakukan pada atribut **category** dan **gender** karena atribut lainnya memiliki jumlah kategori yang sangat banyak sehingga kurang relevan untuk divisualisasikan"""

# Bar plot untuk data kategorikal
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
df_X['category'].value_counts().plot(kind='bar')
plt.title('category')
plt.subplot(1, 2, 2)
df_X['gender'].value_counts().plot(kind='bar')
plt.title('gender')
plt.tight_layout()
plt.show()

"""Berdasarkan diagram batang diatas tampak bahwa transaksi cenderung merata pada berbagai macam **category** dan **gender**

### **Label**
"""

# Pie chart untuk distribusi label
label_count = df_y.value_counts()
label_count.plot(kind='pie', autopct='%1.1f%%')
plt.title('Distribusi Label')
plt.ylabel('')
plt.legend(['Non-Fraud', 'Fraud'])
plt.show()

"""Dataset sangat tidak seimbang, di mana label didominasi kategori non-fraud sebesar 99,4% dan fraud hanya 0,6%

## **Multivariate Analysis**
"""

df_X_trans = df_X.copy()

# Konversi waktu tanggal ke format datetime
df_X_trans['trans_date_trans_time'] = pd.to_datetime(df_X_trans['trans_date_trans_time'])
df_X_trans['dob'] = pd.to_datetime(df_X_trans['dob'])

# Feature engineering kategori jam
df_X_trans['trans_hour'] = df_X_trans['trans_date_trans_time'].dt.hour

# Feature engineering kategori hari
df_X_trans['trans_day'] = df_X_trans['trans_date_trans_time'].dt.dayofweek

# Feature engineering kategori bulan
df_X_trans['trans_month'] = df_X_trans['trans_date_trans_time'].dt.month

# Feature engineering umur
df_X_trans['age'] = df_X_trans['trans_date_trans_time'].dt.year - df_X_trans['dob'].dt.year

"""### **Numerik**"""

# Histogram untuk kolom numerik
cols = ['amt', 'city_pop', 'age']
for col in cols:
  plt.subplot(1, 2, 1)
  df_X_trans[col][df_y == 0].hist(bins=30, figsize=(10, 3), color='blue')
  plt.title(f'{col} (non-fraud)')
  plt.subplot(1, 2, 2)
  df_X_trans[col][df_y == 1].hist(bins=30, figsize=(10, 3), color='orange')
  plt.title(f'{col} (fraud)')
  plt.show()

"""Berdasarkan histogram diatas tampak bahwa transaksi fraud cenderung terjadi pada jumlah pembelian tertentu (**amt**) dan umur pelaku tertentu (**age**)

### **Kategorikal**
"""

# Crosstab ternormalisasi untuk setiap kategori pada data kategorikal
cols = ['category', 'gender', 'trans_hour', 'trans_day', 'trans_month']
for i, cat in enumerate(cols):
  crosstab = pd.crosstab(df_X_trans[cat], df_y)
  crosstab = (crosstab.T / crosstab.sum(axis=1)).T
  crosstab.plot(kind='barh', stacked=True)
  plt.xlim(0.97, 1)
  plt.title(cat)
  plt.show()

# Crosstab untuk data kategorikal
cols = ['category', 'gender', 'trans_hour', 'trans_day', 'trans_month']
for col in cols:
  plt.figure(figsize=(10, 3))
  crosstab = pd.crosstab(df_X_trans[col], df_y)
  plt.subplot(1, 2, 1)
  crosstab.loc[:, 0].plot(kind='bar', color='blue')
  plt.title(f'{col} (non-fraud)')
  plt.subplot(1, 2, 2)
  crosstab.loc[:, 1].plot(kind='bar', color='orange')
  plt.title(f'{col} (fraud)')
  plt.show()

"""Berdasarkan diagram batang diatas tampak bahwa transaksi fraud cenderung terjadi pada **category**, jam (**trans_hour**), dan bulan (**trans_month**) tertentu

## **Discussion**

* Dataset memiliki 1296675 data dengan 24 atribut
* Dataset terdiri dari atribut numerik dan kategorikal
* Dataset tidak memiliki record yang duplikat
* Dataset memiliki missing value pada atribut **merch_zipcode** sebanyak sekitar 15%
* Atribut numerik memiliki outlier tetapi pada atribut posisi, populasi, nomor kartu kredit, dan jumlah transaksi (bukan outlier).
* Sebagian besar transaksi terjadi pada jumlah transaksi (**amt**) yang cenderung rendah dan pada kota dengan populasi (**city_pop**) rendah
* Transaksi cenderung merata pada berbagai macam **category** dan **gender**
* Dataset sangat tidak seimbang, di mana label didominasi kategori non-fraud sebesar 99,4% dan fraud hanya 0,6%
* Transaksi fraud cenderung terjadi pada jumlah pembelian (**amt**), umur pelaku (**age**), **category**, jam (**trans_hour**), dan bulan (**trans_month**) tertentu.

# **Data Preprocessing**

## **Split Data**

Agar performa model lebih valid maka model diuji pada data yang berbeda dari data training
"""

# Split dataset menjadi dataset train dan dataset test
df_train, df_test, label_train, label_test = train_test_split(df_X, df_y, test_size = 0.2, random_state = 42, stratify=df_y)

# Reset index
df_train = df_train.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)
label_train = label_train.reset_index(drop=True)
label_test = label_test.reset_index(drop=True)

# Mencek jumlah kelas di setiap split dataset
dataset_count = pd.concat([label_train.value_counts(),
                           label_test.value_counts()],
                           axis=1,
                           keys=['train', 'test']).T
dataset_count['Total'] = dataset_count.sum(axis=1)
dataset_count['Percentage'] = dataset_count['Total'] / dataset_count['Total'].sum()
dataset_count

"""## **Drop Attributes**

Beberapa atribut di drop dengan penjelasan sebagai berikut
*   **Unnamed: 0 :** indeks transaksi tidak berhubungan dengan penipuan
*   **cc_num -->** terlalu unik
*   **merchant -->** terlalu banyak kategori
*   **first -->** nama dapan tidak berhubungan dengan penipuan
*   **last -->** nama belakang tidak berhubungan dengan penipuan
*   **street -->** terlalu banyak kategori
*   **city -->** terlalu banyak kategori
*   **state -->** terlalu banyak kategori
*   **zip -->** terlalu banyak kategori
*   **lat -->** tidak bisa digunakan secara langsung (posisi unik)
*   **long -->** tidak bisa digunakan secara langsung (posisi unik)
*   **job -->** terlalu banyak kategori
*   **trans_num -->** terlalu unik
*   **unix_time -->** sudah diwakili trans_date_trans_time
*   **merch_lat -->** tidak bisa digunakan secara langsung (posisi unik)
*   **merch_long -->** tidak bisa digunakan secara langsung (posisi unik)
*   **merch_zipcode -->** terlalu banyak kategori
"""

# Drop atribut yang tidak berkorelasi dengan fraud detection
drop_cols = ['Unnamed: 0', 'cc_num', 'merchant', 'first', 'last', 'street', 'city', 'state', 'zip', 'lat', 'long', 'job', 'trans_num', 'unix_time',
             'merch_lat', 'merch_long', 'merch_zipcode']
df_train = df_train.drop(columns=drop_cols)
df_test = df_test.drop(columns=drop_cols)

"""## **Feature Engineering**

Untuk mendapatkan fitur-fitur yang lebih optimal maka disusun fitur baru seperti
* **trans_hour** : jam transaksi
* **trans_day** : hari transaksi
* **trans_month** : bulan transaksi
* **age** : umur pengguna kartu kredit ketika transaksi
"""

class FEATURE_ENGINEERING(BaseEstimator, TransformerMixin):
  def fit(self, df_train, label=None):
    return self

  def transform(self, df):
    df_trans = df.copy()
    # Konversi waktu tanggal ke format datetime
    df_trans['trans_date_trans_time'] = pd.to_datetime(df_trans['trans_date_trans_time'])
    df_trans['dob'] = pd.to_datetime(df_trans['dob'])
    # Feature engineering kategori jam
    df_trans['trans_hour'] = df_trans['trans_date_trans_time'].dt.hour
    # Feature engineering kategori hari
    df_trans['trans_day'] = df_trans['trans_date_trans_time'].dt.dayofweek
    # Feature engineering kategori bulan
    df_trans['trans_month'] = df_trans['trans_date_trans_time'].dt.month
    # Feature engineering umur
    df_trans['age'] = df_trans['trans_date_trans_time'].dt.year - df_trans['dob'].dt.year
    # Drop column
    df_trans = df_trans.drop(columns=['trans_date_trans_time', 'dob'])
    return df_trans

  def fit_transform(self, df, label=None):
    return self.fit(df).transform(df)

fea = FEATURE_ENGINEERING()
df_train = fea.fit_transform(df_train, label_train)
df_test = fea.transform(df_test)

df_train.columns

df_test.columns

"""## **Split Attributes (Numerical, Categorical)**

Memisahkan atribut numerik dan kategorikal karena membutuhkan penanganan yang berbeda
"""

# Mencari atribut numerik dan kategorical
num_cols = ['amt', 'city_pop', 'age']
cat_cols = ['category', 'gender', 'trans_hour', 'trans_day', 'trans_month']

# Split data numerical dan kategorikal
num_train, cat_train = df_train[num_cols].copy(), df_train[cat_cols].copy()
num_test, cat_test = df_test[num_cols].copy(), df_test[cat_cols].copy()

"""## **Normalisasi Data**

Data numerik dinormalisasi untuk menyamakan rentang nilai menjadi antara 0-1
"""

# Feature scaling menggunakan MinMaxScaler
sc_num = MinMaxScaler()
num_train = sc_num.fit_transform(num_train)
num_test = sc_num.transform(num_test)

"""## **Categorical Encoding**

Data kategori diubah menjadi numerik agar dapat dilakukan modelling
"""

# One Hot Encoder untuk data kategorikal
enc = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')
cat_train = enc.fit_transform(cat_train)
cat_test = enc.transform(cat_test)
cat_feature_names = enc.get_feature_names_out()

"""## **Combine Attributes**

Atribut numerik dan kategori digabungkan untuk membentuk satu kesatuan fitur
"""

# Ubah dalam bentuk dataframe
num_train = pd.DataFrame(num_train, columns=num_cols)
num_test = pd.DataFrame(num_test, columns=num_cols)
cat_train = pd.DataFrame(cat_train, columns=cat_feature_names)
cat_test = pd.DataFrame(cat_test, columns=cat_feature_names)

# Gabungkan dengan data numerik, ordinal, dan nominal
df_train = pd.concat([num_train, cat_train], axis=1)
df_test = pd.concat([num_test, cat_test], axis=1)

df_train.head()

df_test.head()

X_train, X_test = df_train.values, df_test.values
y_train, y_test = label_train.values, label_test.values

"""## **Handling Imbalance**

Mengurangi data imbalance dengan oversampling
"""

# Mencek jumlah kelas di setiap split dataset
print("y_train [0 1] :", np.bincount(y_train))
print("y_test  [0 1] :", np.bincount(y_test))

# Oversampling dengan SMOTE
ada = SMOTE(random_state=42, sampling_strategy=0.1)
X_train, y_train = ada.fit_resample(X_train, y_train)

# Mencek jumlah kelas di setiap split dataset
print("y_train [0 1] :", np.bincount(y_train))
print("y_test  [0 1] :", np.bincount(y_test))

"""# **Modeling**

## **Preprocessing**
"""

# Split dataset menjadi dataset train dan dataset test
df_train, df_test, label_train, label_test = train_test_split(df_X, df_y, test_size = 0.2, random_state = 42, stratify=df_y)

# Reset index
df_train = df_train.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)
label_train = label_train.reset_index(drop=True)
label_test = label_test.reset_index(drop=True)

# Drop atribut yang tidak berkorelasi dengan fraud detection
drop_cols = ['Unnamed: 0', 'cc_num', 'merchant', 'first', 'last', 'street', 'city', 'zip', 'lat', 'long', 'job', 'trans_num', 'unix_time',
             'merch_lat', 'merch_long', 'merch_zipcode']
df_train = df_train.drop(columns=drop_cols)
df_test = df_test.drop(columns=drop_cols)

"""## **Pipeline**"""

num_cols = ['amt', 'city_pop', 'age']
cat_cols = ['category', 'gender', 'trans_hour', 'trans_day', 'trans_month']

num_preprocessor = Pipeline(
  steps=[
      ('feature_scaling', MinMaxScaler()),
  ])

cat_preprocessor = Pipeline(
  steps=[
      ('one_hot_encoding', OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')),
    ])

splitter = ColumnTransformer([
    ('numerical', num_preprocessor, num_cols),
    ('categorical', cat_preprocessor, cat_cols),
    ])

pipe_rf = Pipeline(
    steps=[
        ('feature_engineering', FEATURE_ENGINEERING()),
        ('splitter', splitter),
        ('oversampling', SMOTE(random_state=42, sampling_strategy=0.1)),
        ('model', RandomForestClassifier(random_state=42, n_jobs=-1))
    ])

"""## **Evaluasi Model**"""

def score(label, predict):
  results = {}
  results['accuracy'] = accuracy_score(label, predict)
  results['precision'] = precision_score(label, predict)
  results['recall'] = recall_score(label, predict)
  results['f1'] = f1_score(label, predict)
  return results

class GRID_SEARCH():
  def __init__(self, model, params_dict):
    self.model = model
    self.params_keys = params_dict.keys()
    self.params_values = params_dict.values()
    # Membuat semua kombinasi values parameter
    self.params_combinations = list(product(*self.params_values))

  def evaluate(self, df_train, label_train):
    self.reports = {
      'params': [],
      'accuracy_train': [],
      'precision_train': [],
      'recall_train': [],
      'f1_train': [],
      'accuracy_valid': [],
      'precision_valid': [],
      'recall_valid': [],
      'f1_valid': [],
      }
    self.best_model = clone(self.model)

    # Split data menjadi train dan valid
    X_train, X_valid, y_train, y_valid = train_test_split(df_train, label_train, test_size=0.11, random_state=42, stratify=label_train)

    # Evaluasi untuk setiap kombinasi parameter
    for combination in self.params_combinations:
      params = dict(zip(self.params_keys, combination))
      # Atur parameter
      self.model.set_params(**params)
      # Latih model
      self.model.fit(X_train, y_train)
      # Prediksi model
      predict_train = self.model.predict(X_train)
      predict_valid = self.model.predict(X_valid)
      # Score model
      results_train = score(y_train, predict_train)
      results_valid = score(y_valid, predict_valid)
      # Simpan parameter
      self.reports['params'].append(params)
      # Simpan hasil train
      for key, value in results_train.items():
        self.reports[f'{key}_train'].append(value)
      # Simpan hasil valid
      for key, value in results_valid.items():
        self.reports[f'{key}_valid'].append(value)

    # Cari parameter terbaik
    best_params = self.reports['params'][np.argmax(self.reports['f1_valid'])]
    # Atur parameter terbaik pada model
    self.best_model.set_params(**best_params)
    # Train ulang model terbaik dengan seluruh data train awal
    self.best_model.fit(df_train, label_train)

# Fungsi untuk menampilkan
def show_results(gs_model):
  results = pd.DataFrame(gs_model.reports)
  results = results.sort_values(by='f1_valid', ascending=False)
  results = results.reset_index(drop=True)
  return results[['params', 'accuracy_train', 'accuracy_valid', 'precision_valid', 'recall_valid', 'f1_valid']]

"""### **Original**

"""

grid_params = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [None, 20]
}

pipe_rf.set_params(oversampling = 'passthrough')
gs_1 = GRID_SEARCH(pipe_rf, grid_params)
gs_1.evaluate(df_train, label_train)

# Menampilkan hasil evaluasi parameter
show_results(gs_1)

path_folder = '/content/drive/MyDrive/Dicoding/Predictive Analysis/'

# Simpan model terbaik yang sudah dilatih
joblib.dump(gs_1.best_model, path_folder + 'original.joblib')

"""### **SMOTE**"""

grid_params = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [None, 20]
}

pipe_rf.set_params(oversampling = SMOTE(random_state=42, sampling_strategy=0.1))
gs_2 = GRID_SEARCH(pipe_rf, grid_params)
gs_2.evaluate(df_train, label_train)

# Menampilkan hasil evaluasi parameter
show_results(gs_2)

# Simpan model terbaik yang sudah dilatih
joblib.dump(gs_2.best_model, path_folder + 'smote.joblib')

"""### **Cascade**"""

# Memuat pipeline
cascade_1 = joblib.load(path_folder + 'smote.joblib')

# Probabilitas prediksi cascade_1 pada data train
prob = cascade_1.predict_proba(df_train)
# Probabilitas prediksi non-fraud
prob_nf = prob[:, 0]
# Index dengan probabilitas non-fraud < 0.99 (ngga yakin non-fraud)
index_nf = np.where(prob_nf < 0.99)[0]
# Update dataset
df_train_1 = df_train.iloc[index_nf].copy()
label_train_1 = label_train.iloc[index_nf].copy()

# Pie chart untuk distribusi label pada data train yang sudah diupdate
label_count = label_train_1.value_counts()
label_count.plot(kind='pie', autopct='%1.1f%%')
plt.title('Distribusi Label')
plt.ylabel('')
plt.legend(['Non-Fraud', 'Fraud'])
plt.show()

# Evaluasi model random forest dengan data latih yang telah di update
grid_params = {
    'model__n_estimators': [100, 200],
    'model__max_depth': [None, 20]
}

# Ubah oversampling strategy menjadi auto karena perbedaan label positif dan negatif sudah tidak terlampau jauh
pipe_rf.set_params(oversampling = SMOTE(random_state=42, sampling_strategy='auto'))
gs_3 = GRID_SEARCH(pipe_rf, grid_params)
gs_3.evaluate(df_train_1, label_train_1)

# Menampilkan hasil evaluasi parameter
show_results(gs_3)

# Simpan model terbaik yang sudah dilatih
joblib.dump(gs_3.best_model, path_folder + 'cascade_2.joblib')

"""## **Test Best Model**"""

scores = {}

"""### **Original**"""

# Memuat model random forest original terbaik
ori_model = joblib.load(path_folder + 'original.joblib')

# Prediksi model pada data test
predict = ori_model.predict(df_test)

# Confusion matrix model
conf_mat = confusion_matrix(label_test, predict)
displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=ori_model.classes_)
displ.plot()
plt.show()

# Hasil pengujian model pada data test
scores['Original'] = score(label_test, predict)
pd.DataFrame(scores['Original'], index=['Original'])

"""### **SMOTE**"""

# Memuat model random forest dengan smote terbaik
smote_model = joblib.load(path_folder + 'smote.joblib')

# Prediksi model pada data test
predict = smote_model.predict(df_test)

# Confusion matrix model
conf_mat = confusion_matrix(label_test, predict)
displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=smote_model.classes_)
displ.plot()
plt.show()

# Hasil pengujian model pada data test
scores['SMOTE'] = score(label_test, predict)
pd.DataFrame(scores['SMOTE'], index=['SMOTE'])

"""### **Cascade**"""

class CASCADE():
  def __init__(self, cascade_1, cascade_2, threshold):
    self.cascade_1 = cascade_1
    self.cascade_2 = cascade_2
    self.threshold = threshold
    self.classes_ = self.cascade_1.classes_

  def predict(self, df):
    # Probabilitas prediksi cascade_1
    prob_1 = self.cascade_1.predict_proba(df)
    # Probabilitas prediksi non-fraud (0)
    prob_nf_1 = prob_1[:, 0]
    # Prediksi cascade 2
    pred_2 = self.cascade_2.predict(df)
    # Menggabungkan hasil prediksi
    pred = np.where(prob_nf_1 >= self.threshold,
                    0,
                    pred_2)
    return pred

# Memuat model
cascade_1 = joblib.load(path_folder + 'smote.joblib')
cascade_2 = joblib.load(path_folder + 'cascade_2.joblib')

# Prediksi model cascade pada data test
cas = CASCADE(cascade_1, cascade_2, threshold=0.99)
predict = cas.predict(df_test)

# Confusion matrix model cascade
conf_mat = confusion_matrix(label_test, predict)
displ = ConfusionMatrixDisplay(confusion_matrix=conf_mat, display_labels=cas.classes_)
displ.plot()
plt.show()

# Hasil pengujian model cascade pada data test
scores['Cascade'] = score(label_test, predict)
pd.DataFrame(scores['Cascade'], index=['Cascade'])

"""## **Results**"""

reports = pd.DataFrame(scores).T
reports.plot(kind='bar')
plt.ylim(0.70, 1)
plt.title('Model Performance')
plt.xticks(rotation=0)
plt.show()

reports

"""Berdasarkan pengujian yang telah dilakukan model random forest original memiliki akurasi 99,83%, presisi 97,07%, recall 72,95%, dan f1-score 83,30%. Kemudian, model random forest dengan SMOTE memiliki akurasi 99,83%, presisi 90,49%, recall 79,21%, dan f1-score 84,48%. Sementara itu, model cascade random forest memiliki akurasi 99,85%, presisi 93,26%, recall 80,21%, dan f1-score 86,25%. Hal ini berarti model cascade mampu melampaui model lainnya, khususnya pada f1-score. Peningkatan pada f1-score mencapai 2,95% dibandingkan model random forest original dan 1,77% dibandingkan model random forest dengan SMOTE. Peningkatan pada F1-score tersebut penting karena menawarkan keseimbangan tidak hanya pada pengurangan false positive tetapi juga false negative. Dengan demikian, model dapat menjaga pengalaman pengguna dan mengurangi kerugian dari segi bisnis."""